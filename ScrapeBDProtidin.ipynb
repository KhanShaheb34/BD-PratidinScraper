{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "ScrapeBDProtidin",
      "provenance": [],
      "collapsed_sections": [],
      "toc_visible": true,
      "authorship_tag": "ABX9TyO+3WMkcWFzbIBHMJIr1la4",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/KhanShaheb34/BD-ProtidinScraper/blob/master/ScrapeBDProtidin.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "me74CKUr7exw",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import bs4\n",
        "import requests\n",
        "import os\n",
        "import pandas as pd\n",
        "import re\n",
        "from datetime import timedelta, datetime"
      ],
      "execution_count": 1,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "u0VnAANoTKlw",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def getNewsFromCategoryAndDate(category, date):\n",
        "  url = f\"https://www.bd-pratidin.com/{category}/{date}\"\n",
        "  page = requests.get(url)\n",
        "  soup = bs4.BeautifulSoup(page.content,\"html.parser\").find(\"div\",{\"class\": \"container-left-area col-md-9\"})\n",
        "  newsLinks = []\n",
        "\n",
        "  for a in soup.findAll(\"a\", {\"href\": re.compile(r\"^[a-z-]+\\/\\d+\\/\\d+\\/\\d+\\/\\d+$\")}):\n",
        "    newsLinks.append(a[\"href\"])\n",
        "\n",
        "  return newsLinks"
      ],
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Ehc4rv40ZQ4N",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def getNewsFromLink(link):\n",
        "  url = f\"https://www.bd-pratidin.com/{link}\"\n",
        "  page = requests.get(url)\n",
        "  soup = bs4.BeautifulSoup(page.content, \"html.parser\")\n",
        "\n",
        "  title = soup.find(\"h1\", {\"class\":\"post-title\"}).text.strip()\n",
        "  description = soup.find(\"meta\",{\"property\":\"og:description\"})[\"content\"].strip()\n",
        "  category = link.split(\"/\")[0]\n",
        "  id = int(link.split(\"/\")[4])\n",
        "  date = \"/\".join(link.split(\"/\")[1:4])\n",
        "\n",
        "  articleSoup = soup.find(\"article\")\n",
        "  article = \"\"\n",
        "  for p in articleSoup.findAll(\"p\"):\n",
        "    article += p.text\n",
        "\n",
        "  return {\"id\": id,\n",
        "          \"title\": title, \n",
        "          \"description\": description, \n",
        "          \"category\": category,\n",
        "          \"date\": date,\n",
        "          \"article\": article}"
      ],
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_djSk_Kp7j5Z",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def getNewsFromDate(date, save=0, verbose=1):\n",
        "  url = f\"https://www.bd-pratidin.com/archive/{date}\"\n",
        "  page = requests.get(url)\n",
        "  soup = bs4.BeautifulSoup(page.content, \"html.parser\").find(\"div\", {\"class\": \"container-left-area printversion col-md-9\"})\n",
        "  categories = set()\n",
        "\n",
        "  for a in soup.findAll(\"a\", {\"href\": re.compile(r\"^[a-z-]+\\/\\d+\\/\\d+\\/\\d+\")}):\n",
        "    categories.add(a[\"href\"].split(\"/\")[0])\n",
        "  categories = list(categories)\n",
        "\n",
        "  if verbose==1:\n",
        "    print(f\"There are {len(categories)} categories.\")\n",
        "\n",
        "  news = []\n",
        "  for category in categories:\n",
        "    newsLink = getNewsFromCategoryAndDate(category, date)\n",
        "\n",
        "    if verbose==1:\n",
        "      print(f\"Downloading {len(newsLink)} news from '{category}' category...\")\n",
        "\n",
        "    for link in newsLink:\n",
        "      news.append(getNewsFromLink(link))\n",
        "\n",
        "    if verbose==1:\n",
        "      print(f\"Done!\")\n",
        "\n",
        "  news_df = pd.DataFrame(news)\n",
        "\n",
        "  if save != 0:\n",
        "    news_df.to_csv(save, index=False)\n",
        "\n",
        "  return news_df"
      ],
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zw5jH4kpW3kT",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def saveNewsFromMultipleDate(days=0, startDate=datetime.today(), verbose=1):\n",
        "  for day in range(days+1):\n",
        "    date = (startDate - timedelta(days=day)).strftime(\"%Y/%m/%d\")\n",
        "    filename = \"-\".join(date.split(\"/\")) + \".csv\"\n",
        "\n",
        "    if verbose==1:\n",
        "      print(f\"Saving news from {date} in {filename}...\")\n",
        "\n",
        "    getNewsFromDate(date, filename, verbose)\n",
        "\n",
        "    if verbose==1:\n",
        "      print(f\"Saving news from {date} in {filename} is complete.\\n\")"
      ],
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Is13OV3sex9x",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 646
        },
        "outputId": "9f78c642-97cb-4202-b47a-0eb89912b2de"
      },
      "source": [
        "saveNewsFromMultipleDate(10)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Saving news from 2020/09/02 in 2020-09-02.csv...\n",
            "There are 11 categories.\n",
            "Downloading 6 news from 'editorial' category...\n",
            "Done!\n",
            "Downloading 23 news from 'country-village' category...\n",
            "Done!\n",
            "Downloading 15 news from 'sport-news' category...\n",
            "Done!\n",
            "Downloading 26 news from 'last-page' category...\n",
            "Done!\n",
            "Downloading 12 news from 'international' category...\n",
            "Done!\n",
            "Downloading 5 news from 'news' category...\n",
            "Done!\n",
            "Downloading 16 news from 'first-page' category...\n",
            "Done!\n",
            "Downloading 9 news from 'entertainment-news' category...\n",
            "Done!\n",
            "Downloading 1 news from 'horoscope' category...\n",
            "Done!\n",
            "Downloading 30 news from 'city' category...\n",
            "Done!\n",
            "Downloading 5 news from 'various-lifestyles' category...\n",
            "Done!\n",
            "Saving news from 2020/09/02 in 2020-09-02.csv is complete.\n",
            "\n",
            "Saving news from 2020/09/01 in 2020-09-01.csv...\n",
            "There are 12 categories.\n",
            "Downloading 6 news from 'editorial' category...\n",
            "Done!\n",
            "Downloading 24 news from 'country-village' category...\n",
            "Done!\n",
            "Downloading 5 news from 'various-city-roundup' category...\n",
            "Done!\n",
            "Downloading 13 news from 'sport-news' category...\n",
            "Done!\n",
            "Downloading 17 news from 'last-page' category...\n"
          ],
          "name": "stdout"
        }
      ]
    }
  ]
}